# -*- coding: utf-8 -*-
"""fullCode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BeG_vv9fIZE93nlV24A9Fyk8VUQug9Tv

#**daimonds project**

firstly 
get data
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import tarfile
import urllib
import pandas as pd
import urllib.request
import numpy as np

import matplotlib.pyplot as plt
# %matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

#dim=pd.read_csv('diamonds.csv')
#dim.drop('Unnamed: 0',axis=1, inplace=True)
#dim.head()

import io
from google.colab import files
uploaded = files.upload()
data = io.BytesIO(uploaded['diamonds.csv'])

dim=pd.read_csv('diamonds.csv')
dim.drop('Unnamed: 0',axis=1, inplace=True)
dim.head()#to show first 5 col

dim #to show all data

dim.info()#give information about data

dim.describe()#Xبحسب جميع انواع العمليات الحسابية

dim.isna().sum() # check for missing values in data.

"""#**Matplotlib**"""

w=dim['price']
c=dim['carat']
plt.bar(w,c)

"""1-histogram"""

dim.hist(bins=50, figsize=(20,15))
plt.show()

"""2-pair plot"""

sns.pairplot(data=dim)
plt.show()

"""3-join plot"""

sns.jointplot(data=dim,y='carat',x='price',hue='color')
plt.show()
sns.jointplot(data=dim,y='x',x='price',hue='color')
plt.show()
sns.jointplot(data=dim,y='y',x='price',hue='color')
plt.show()
sns.jointplot(data=dim,y='z',x='price',hue='color')
plt.show()
sns.jointplot(data=dim,y='c',x='price',hue='color')
plt.show()

"""4-histplot """

sns.histplot(data=dim,x='carat')
plt.show()
sns.histplot(data=dim,x='price')
plt.show()
sns.histplot(data=dim,x='carat',y='price',hue='color')
plt.show()
sns.histplot(data=dim,x='depth',y='price',hue='color')
plt.show()

sns.histplot(data=dim,x='x',y='price',hue='color')
plt.show()
sns.histplot(data=dim,x='y',y='price',hue='color')
plt.show()
sns.histplot(data=dim,x='z',y='price',hue='color')
plt.show()
sns.histplot(data=dim,x='carat',y='price',hue='clarity')
plt.show()

"""5-heatmap"""

corr=dim.corr()
corr

sns.heatmap(dim.corr(),annot=True,cmap='flare')
plt.show()

"""6-box plot"""

sns.boxplot(data=dim,orient="h")
plt.show()
sns.boxplot(x="carat",data=dim,orient="h")
plt.show()
sns.boxplot(x="x",data=dim,orient="h")
plt.show()
sns.boxplot(x="y",data=dim,orient="h")
plt.show()
sns.boxplot(x="z",data=dim,orient="h")
plt.show()
sns.boxplot(x="table",data=dim,orient="h")
plt.show()
sns.boxplot(x="depth",data=dim,orient="h")
plt.show()

dim.color.value_counts()# بحسب كل لون كم عدده

dim.depth.value_counts()

dim.price.value_counts()

dim.carat.value_counts()

dim.clarity.value_counts()

"""#**outliers**

"""

Q1 = dim['depth'].quantile(0.25)
Q3 = dim['depth'].quantile(0.75)
IQR = Q3 - Q1
idx = ~((dim['depth'] < (Q1 - 1.5 * IQR)) | (dim['depth'] > (Q3 + 1.5 * IQR)))
dim[idx] # Clean Dataset

"""#**Onehotencoding for Categorical variables**



"""

#cat_cols = dim.select_dtypes(include='object').columns.to_list() # OHE using Pandas
#dim1=pd.get_dummies(dim[idx], columns=cat_cols, drop_first=True)

#dim1

"""#**#Using OrdinalEncoder to transform categorical values**"""

#from sklearn.preprocessing import OrdinalEncoder
#enc = OrdinalEncoder()
#dim[["cut", "color","clarity"]] = enc.fit_transform(dim[["cut", "color","clarity"]])
##dim[array(['Fair','Good', 'Very Good', 'Premium', 'Ideal'], dtype=object),array(['G','E','F','H','D','I','J'], dtype=object),array(['I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF'], dtype=object)]

#dim

#dim.clarity.value_counts()

dim1=dim

dim['cut'].unique()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

dim1['cut'] = le.fit_transform(dim1['cut'])
dim1['color'] = le.fit_transform(dim1['color'])
dim1['clarity'] = le.fit_transform(dim1['clarity'])

dim1

dim['cut'].unique()

"""#**Train data & test data**"""

from sklearn.model_selection import train_test_split

X=dim1.drop('price', axis=1)
y=dim1['price']
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)
X_train

"""#**Standardization**"""

from sklearn.preprocessing import StandardScaler
#creating Polynomial features as there is some degree of variation in the linear relationship
#scaler = PolynomialFeatures(degree=2, interaction_only=True)

scaler = StandardScaler()
X_train = StandardScaler().fit_transform(X_train)
X_test = StandardScaler().fit_transform(X_test)
X_train.mean(axis = 0)#Verify that the mean of each feature (column) is 0:

"""# 1- Select and Train a Model

pipline
"""

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

#index for all col
carat_ix,depth_ix, table_ix, price_ix,x_ix,y_ix,z_ix = [
    list(dim1.columns).index(col)
    for col in ("carat", "depth", "table", "price","x","y","z")]


#class CombinedAttributesAdder(BaseEstimator, TransformerMixin):

"""#LinearRegression model"""

from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
pred=lin_reg.predict(X_test)
pred
#LinearRegression(copy_X=True ,fit_intercept=True, n_jobs=None, normalize=False)

lin_reg.score(X_test,y_test)

"""#**decision tree**"""

from sklearn.tree import DecisionTreeRegressor 
tree_reg = DecisionTreeRegressor()
tree_reg.fit(X_train, y_train)
tree_predict = tree_reg.predict(X_test)

"""#**Calculating scores for Decision Tree using CrossValidation**"""

from sklearn.model_selection import cross_val_score
scores = cross_val_score(tree_reg, X_test, y_test, scoring = "neg_mean_squared_error", cv = 10)
tree_rmse_scores = np.sqrt(-scores)
print ("Scores: ", tree_rmse_scores)
print ("Mean: ", tree_rmse_scores.mean())
print ("Standard Deviation: ", tree_rmse_scores.std())
from sklearn.metrics import r2_score
print("R^2 error: ", r2_score(y_test, tree_predict))

"""#**random forest**"""

from sklearn.ensemble import RandomForestRegressor
rand_for=RandomForestRegressor()
rand_for.fit(X_train,y_train)
scores = cross_val_score(rand_for, X_test, y_test, scoring = "neg_mean_squared_error", cv = 10)
rand_for_scores = np.sqrt(-scores)
print ("Scores: ", rand_for_scores)
print ("Mean: ", rand_for_scores.mean())
print ("Standard Deviation: ", rand_for_scores.std())

rand_for.score(X_test,y_test)

predicted_train=rand_for.predict(X_train)
predicted_test=rand_for.predict(X_test)

predicted_test

predicted_train

"""#**RMES/MES**"""



#call RMSE model & MSE
from sklearn.metrics import mean_squared_error
MSE1=lin_reg.predict(X_train)
lin_mse = mean_squared_error(y_train, MSE1)
print(lin_mse)
lin_rmse = np.sqrt(lin_mse)
lin_rmse

MSE2=rand_for.predict(X_test)
lin_mse1 = mean_squared_error(y_test, MSE2)
print(lin_mse1)
lin_rmse1 = np.sqrt(lin_mse1)
lin_rmse1

"""#**Save every model you experiment with using the joblib library**

# Hyperparametrization

#**We start with Random Search to find ideal hyperparameters to use with GridSearch for Random Forest Regression.**
"""

from sklearn.model_selection import RandomizedSearchCV
import numpy as np
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
max_features = ['auto', 'sqrt']
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
# These are the most important hyperparameters for Random Forest Regression.
rf_random = RandomizedSearchCV(estimator = rand_for, param_distributions = random_grid, 
                               n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
rf_random.fit(X_train, y_train)

def evaluate(model, test_features, test_labels):
    predictions = model.predict(test_features)
    errors = abs(predictions - test_labels)
    mape = 100 * np.mean(errors / test_labels)
    accuracy = 100 - mape
    print('Model Performance')
    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))
    print('Accuracy = {:0.2f}%.'.format(accuracy))
    
    return accuracy

base_model = RandomForestRegressor()
base_model.fit(X_train, y_train)
base_accuracy = evaluate(base_model, X_test, y_test)

best_random = rf_random.best_estimator_
random_accuracy = evaluate(best_random, X_test, y_test)

"""#Grid Search"""

rf_random.best_params_

from sklearn.model_selection import GridSearchCV
rf = RandomForestRegressor()
param_grid = {
    'bootstrap': [True],
    'max_depth': [80, 90, 100, 110,120],
    'max_features': [2, 3],
    'min_samples_leaf': [1, 2, 3, 4],
    'min_samples_split': [1, 2, 3,4],
    'n_estimators': [600, 700, 800, 900, 1000, 1500]
}
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, 
                          cv = 5, n_jobs = -1, verbose = 2)
grid_search.fit(X_train, y_train)

best_grid = grid_search.best_estimator_
grid_accuracy = evaluate(best_grid, X_test, y_test) # Best estimator was the Random Search Estimator.

best_random = rf_random.best_estimator_
random_accuracy = evaluate(best_random, X_test, y_test)